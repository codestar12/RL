# High-Performance DTensor SFT Config for Team Impossible
# Target: Llama 8B on 8x H100 80GB (single node)
# Usage: uv run python examples/run_sft.py --config team/kai/sft_config.yaml
#
# Note: Using DTensor/FSDP2 backend instead of Megatron because
# CUDA toolkit is not installed on this machine (only runtime libraries).
# Megatron requires compiling CUDA extensions which needs nvcc.
#
# Key Performance Optimizations:
# 1. BF16 training with FSDP2
# 2. Sequence packing for high MFU
# 3. Full sharding for memory efficiency

defaults: ../../examples/configs/sft.yaml

sft:
  max_num_epochs: 1
  max_num_steps: 50000  # Will be bounded by epoch size
  val_period: 500
  val_batches: 8
  val_global_batch_size: 128
  val_micro_batch_size: 2
  val_at_start: true
  seed: 42

checkpointing:
  enabled: true
  checkpoint_dir: results/team-impossible/sft-llama8b-bf16
  metric_name: "val:val_loss"
  higher_is_better: false
  keep_top_k: 5
  save_period: 1000
  checkpoint_must_save_by: null

policy:
  # Source checkpoint in HF format
  model_name: s3://datology-research/cody/torchtitan_ckpts/math_and_code/v2/8b/phase3_1T/hf/step-127120/

  tokenizer:
    # Use Llama 3.1 8B Instruct tokenizer for chat template
    name: meta-llama/Llama-3.1-8B-Instruct
    chat_template: default
    chat_template_kwargs: null

  # Batch configuration
  # GBS=256, DP=8 (FSDP), MBS=4 -> grad_accum=8
  train_global_batch_size: 256
  train_micro_batch_size: 4

  # Sequence length
  max_total_sequence_length: 4096

  # Precision - BF16 for DTensor
  precision: "bfloat16"

  offload_optimizer_for_logprob: false

  # DTensor/FSDP backend (V1 - uses native PyTorch, doesn't require nemo_automodel)
  dtensor_cfg:
    enabled: true
    _v2: false  # Use V1 which doesn't require nemo_automodel
    tensor_parallel_size: 1
    context_parallel_size: 1
    sequence_parallel: false
    cpu_offload: false
    activation_checkpointing: true  # Enable to save memory
    env_vars:
      PYTORCH_CUDA_ALLOC_CONF: "expandable_segments:True"
    custom_parallel_plan: null
    lora_cfg:
      enabled: false

  # Disable Megatron (requires CUDA toolkit)
  megatron_cfg:
    enabled: false

  # Sequence packing - CRITICAL FOR MFU
  sequence_packing:
    enabled: true
    train_mb_tokens: ${mul:${policy.max_total_sequence_length}, ${policy.train_micro_batch_size}}
    algorithm: "modified_first_fit_decreasing"
    sequence_length_round: 64

  dynamic_batching:
    enabled: false

  make_sequence_length_divisible_by: 1

  max_grad_norm: 1.0

  # FSDP optimizer
  optimizer:
    type: "AdamW"
    lr: 2.0e-5
    betas: [0.9, 0.95]
    eps: 1.0e-8
    weight_decay: 0.01

data:
  max_input_seq_length: ${policy.max_total_sequence_length}
  add_bos: true
  add_eos: true
  add_generation_prompt: false  # For SFT, we include response
  shuffle: true
  num_workers: 4

  # OpenAI format dataset prepared by Mira
  # ~584K train samples, ~31K validation samples
  train_data_path: /home/claude/code/RL/team/mira/data/train.jsonl
  val_data_path: /home/claude/code/RL/team/mira/data/val.jsonl
  chat_key: messages

logger:
  log_dir: logs/team-impossible/sft-llama8b-bf16
  wandb_enabled: true
  tensorboard_enabled: true
  mlflow_enabled: false
  swanlab_enabled: false
  monitor_gpus: true

  wandb:
    project: team-impossible
    name: sft-llama8b-bf16-yolo

  tensorboard:
    log_dir: tb_logs-team-impossible-sft

  gpu_monitoring:
    collection_interval: 30
    flush_interval: 60

cluster:
  gpus_per_node: 8
  num_nodes: 1
